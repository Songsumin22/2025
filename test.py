# app.py
# ì‹¤í–‰: streamlit run app.py

import streamlit as st
import feedparser
import requests
from urllib.parse import quote, urlparse
from bs4 import BeautifulSoup
from readability import Document
from datetime import datetime, timezone
from dateutil import tz
import pandas as pd

# ---- ìš”ì•½ (Sumy - LexRank) ----
from sumy.parsers.plaintext import PlainTextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer

# --------- ê¸°ë³¸ ì„¤ì • ----------
st.set_page_config(
    page_title="ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìš”ì•½ ì›¹",
    page_icon="ğŸ—ï¸",
    layout="wide"
)

KST = tz.gettz("Asia/Seoul")

# --------- ìœ í‹¸ ----------
def to_kst(dt: datetime) -> datetime:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(KST)

def reltime(dt: datetime) -> str:
    dt = to_kst(dt)
    delta = datetime.now(KST) - dt
    s = int(delta.total_seconds())
    if s < 60: return f"{s}ì´ˆ ì „"
    m = s // 60
    if m < 60: return f"{m}ë¶„ ì „"
    h = m // 60
    if h < 24: return f"{h}ì‹œê°„ ì „"
    d = h // 24
    return f"{d}ì¼ ì „"

def clean_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()
    text = soup.get_text("\n")
    lines = [ln.strip() for ln in text.splitlines()]
    text = "\n".join([ln for ln in lines if ln])
    return text

@st.cache_data(show_spinner=False, ttl=300)
def fetch_article_text(url: str) -> str:
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        doc = Document(res.text)
        html = doc.summary(html_partial=True)
        text = clean_text(html)
        if len(text) < 400:
            text = clean_text(res.text)
        return text
    except Exception:
        return ""

def lexrank_summary(text: str, n_sentences: int = 3, language: str = "korean") -> list[str]:
    if not text or len(text.split()) < 30:
        return []
    parser = PlainTextParser.from_string(text, Tokenizer(language))
    summarizer = LexRankSummarizer()
    sentences = summarizer(parser.document, n_sentences)
    return [str(s) for s in sentences]

def google_news_rss_url(query: str = "", topic: str = "", lang: str = "ko", country: str = "KR") -> str:
    base = "https://news.google.com/rss"
    if query:
        return f"{base}/search?q={quote(query)}&hl={lang}&gl={country}&ceid={country}%3A{lang}"
    topic_map = {
        "í—¤ë“œë¼ì¸": "",
        "êµ­ì œ": "WORLD",
        "í•œêµ­": "NATION",
        "ë¹„ì¦ˆë‹ˆìŠ¤Â·ê²½ì œ": "BUSINESS",
        "ê³¼í•™": "SCIENCE",
        "ê¸°ìˆ Â·IT": "TECHNOLOGY",
        "ì—”í„°í…Œì¸ë¨¼íŠ¸": "ENTERTAINMENT",
        "ìŠ¤í¬ì¸ ": "SPORTS",
        "ê±´ê°•": "HEALTH",
    }
    code = topic_map.get(topic, "")
    if code:
        return f"{base}/headlines/section/topic/{code}?hl={lang}&gl={country}&ceid={country}%3A{lang}"
    else:
        return f"{base}?hl={lang}&gl={country}&ceid={country}%3A{lang}"

@st.cache_data(show_spinner=False, ttl=120)
def fetch_feed(url: str):
    feed = feedparser.parse(url)
    items = []
    for e in feed.entries:
        title = e.get("title", "").strip()
        link = e.get("link", "")
        published_dt = None
        if "published_parsed" in e and e.published_parsed:
            published_dt = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
        elif "updated_parsed" in e and e.updated_parsed:
            published_dt = datetime(*e.updated_parsed[:6], tzinfo=timezone.utc)
        summary = BeautifulSoup(e.get("summary", ""), "html.parser").get_text(" ")
        source = ""
        if "source" in e and e.source and e.source.get("title"):
            source = e.source.title
        elif link:
            source = urlparse(link).netloc.replace("www.", "")
        items.append({
            "title": title,
            "link": link,
            "summary": summary.strip(),
            "source": source,
            "published": published_dt or datetime.now(timezone.utc),
        })
    items.sort(key=lambda x: x["published"], reverse=True)
    return items

# --------- ì‚¬ì´ë“œë°” ----------
with st.sidebar:
    st.title("ğŸ—ï¸ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìš”ì•½")
    mode = st.radio("ëª¨ë“œ", ["í† í”½ íƒìƒ‰", "í‚¤ì›Œë“œ ê²€ìƒ‰"], horizontal=True)
    if mode == "í† í”½ íƒìƒ‰":
        topic = st.selectbox("ì¹´í…Œê³ ë¦¬",
            ["í—¤ë“œë¼ì¸","í•œêµ­","êµ­ì œ","ë¹„ì¦ˆë‹ˆìŠ¤Â·ê²½ì œ","ê¸°ìˆ Â·IT","ê³¼í•™","ìŠ¤í¬ì¸ ","ì—”í„°í…Œì¸ë¨¼íŠ¸","ê±´ê°•"],
            index=0
        )
        rss = google_news_rss_url(topic=topic)
    else:
        query = st.text_input("ê²€ìƒ‰ì–´", value="AI")
        rss = google_news_rss_url(query=query)

    n_show = st.slider("í‘œì‹œí•  ê¸°ì‚¬ ê°œìˆ˜", 3, 30, 10)
    n_sum = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", 2, 7, 3)
    bullet_mode = st.checkbox("ë¶ˆë¦¿ í˜•ì‹ ìš”ì•½", value=True)

# --------- ë©”ì¸ ----------
st.markdown("## ğŸ—ï¸ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìš”ì•½ ì›¹")

with st.spinner("ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
    items = fetch_feed(rss)
    cards = items[:n_show]

export_rows = []
for it in cards:
    with st.container(border=True):
        st.markdown(f"### [{it['title']}]({it['link']})")
        st.caption(f"{it['source']} â€¢ {reltime(it['published'])}")

        with st.spinner("ë³¸ë¬¸ ìš”ì•½ ì¤‘..."):
            article_text = fetch_article_text(it["link"])
            summary_sents = lexrank_summary(article_text, n_sum, language="korean")

        if summary_sents:
            if bullet_mode:
                st.markdown("\n".join([f"- {s}" for s in summary_sents]))
            else:
                st.write(" ".join(summary_sents))
        else:
            st.info(it["summary"] or "ìš”ì•½ ë¶ˆê°€")

        export_rows.append({
            "title": it["title"],
            "link": it["link"],
            "source": it["source"],
            "published": to_kst(it["published"]).strftime("%Y-%m-%d %H:%M"),
            "summary": " ".join(summary_sents) if summary_sents else it["summary"],
        })

df = pd.DataFrame(export_rows)
csv = df.to_csv(index=False).encode("utf-8-sig")
st.download_button("ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ", csv, "news_summary.csv", "text/csv")

st.caption("âš ï¸ êµìœ¡ìš© ë°ëª¨. ì¼ë¶€ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ëŠ” ë³¸ë¬¸ ì¶”ì¶œì´ ì•ˆ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
