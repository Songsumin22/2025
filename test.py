# app.py
# ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìš”ì•½ ì›¹ (K-ë²„ì „)
# ì‹¤í–‰: streamlit run app.py

import streamlit as st
import feedparser
import requests
from urllib.parse import quote, urlparse
from bs4 import BeautifulSoup
from readability import Document
from datetime import datetime, timezone
from dateutil import tz
from typing import List, Dict
import pandas as pd

# ---- ìš”ì•½ (Sumy - LexRank) ----
from sumy.parsers.plaintext import PlainTextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer

# --------- ê¸°ë³¸ ì„¤ì • ----------
st.set_page_config(
    page_title="ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìš”ì•½ ì›¹",
    page_icon="ğŸ—ï¸",
    layout="wide"
)

KST = tz.gettz("Asia/Seoul")

# --------- ìœ í‹¸ ----------
def to_kst(dt: datetime) -> datetime:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(KST)

def reltime(dt: datetime) -> str:
    dt = to_kst(dt)
    delta = datetime.now(KST) - dt
    s = int(delta.total_seconds())
    if s < 60: return f"{s}ì´ˆ ì „"
    m = s // 60
    if m < 60: return f"{m}ë¶„ ì „"
    h = m // 60
    if h < 24: return f"{h}ì‹œê°„ ì „"
    d = h // 24
    return f"{d}ì¼ ì „"

def clean_text(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    # ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()
    text = soup.get_text("\n")
    lines = [ln.strip() for ln in text.splitlines()]
    text = "\n".join([ln for ln in lines if ln])
    return text

@st.cache_data(show_spinner=False, ttl=300)
def fetch_article_text(url: str) -> str:
    """ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ(readability -> bs4 í´ë¦°ì—…)"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/118.0 Safari/537.36"
    }
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        doc = Document(res.text)
        html = doc.summary(html_partial=True)
        text = clean_text(html)
        # ë§Œì•½ ë½‘íŒ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ
        if len(text) < 400:
            text = clean_text(res.text)
        return text
    except Exception:
        return ""

def lexrank_summary(text: str, n_sentences: int = 3, language: str = "korean") -> List[str]:
    """ì–¸ì–´ì— ìƒê´€ì—†ì´ ëŒ€ì²´ë¡œ ë™ì‘(í•œêµ­ì–´ë„ OK)í•˜ëŠ” LexRank ê¸°ë°˜ ë¬¸ì¥ ì¶”ì¶œ ìš”ì•½"""
    if not text or len(text.split()) < 30:
        return []
    parser = PlainTextParser.from_string(text, Tokenizer(language))
    summarizer = LexRankSummarizer()
    sentences = summarizer(parser.document, n_sentences)
    return [str(s) for s in sentences]

def google_news_rss_url(
    query: str = "",
    topic: str = "",
    lang: str = "ko",
    country: str = "KR"
) -> str:
    base = "https://news.google.com/rss"
    if query:
        # ê²€ìƒ‰ ëª¨ë“œ
        return f"{base}/search?q={quote(query)}&hl={lang}&gl={country}&ceid={country}%3A{lang}"
    # í† í”½ ëª¨ë“œ
    topic_map = {
        "í—¤ë“œë¼ì¸": "",            # ê¸°ë³¸
        "êµ­ì œ": "WORLD",
        "í•œêµ­": "NATION",
        "ë¹„ì¦ˆë‹ˆìŠ¤Â·ê²½ì œ": "BUSINESS",
        "ê³¼í•™": "SCIENCE",
        "ê¸°ìˆ Â·IT": "TECHNOLOGY",
        "ì—”í„°í…Œì¸ë¨¼íŠ¸": "ENTERTAINMENT",
        "ìŠ¤í¬ì¸ ": "SPORTS",
        "ê±´ê°•": "HEALTH",
    }
    code = topic_map.get(topic, "")
    if code:
        return f"{base}/headlines/section/topic/{code}?hl={lang}&gl={country}&ceid={country}%3A{lang}"
    else:
        return f"{base}?hl={lang}&gl={country}&ceid={country}%3A{lang}"

@st.cache_data(show_spinner=False, ttl=120)
def fetch_feed(url: str) -> List[Dict]:
    feed = feedparser.parse(url)
    items = []
    for e in feed.entries:
        title = e.get("title", "").strip()
        link = e.get("link", "")
        published_dt = None
        if "published_parsed" in e and e.published_parsed:
            published_dt = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
        elif "updated_parsed" in e and e.updated_parsed:
            published_dt = datetime(*e.updated_parsed[:6], tzinfo=timezone.utc)
        summary = BeautifulSoup(e.get("summary", ""), "html.parser").get_text(" ")
        source = ""
        if "source" in e and e.source and e.source.get("title"):
            source = e.source.title
        elif link:
            source = urlparse(link).netloc.replace("www.", "")
        items.append({
            "title": title,
            "link": link,
            "summary": summary.strip(),
            "source": source,
            "published": published_dt or datetime.now(timezone.utc),
        })
    # ìµœì‹ ìˆœ ì •ë ¬
    items.sort(key=lambda x: x["published"], reverse=True)
    return items

def make_cards(items: List[Dict], max_items: int) -> List[Dict]:
    return items[:max_items]

# --------- ì‚¬ì´ë“œë°” ----------
with st.sidebar:
    st.title("ğŸ—ï¸ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìš”ì•½")
    mode = st.radio("ëª¨ë“œ", ["í† í”½ íƒìƒ‰", "í‚¤ì›Œë“œ ê²€ìƒ‰"], horizontal=True)
    if mode == "í† í”½ íƒìƒ‰":
        topic = st.selectbox(
            "ì¹´í…Œê³ ë¦¬",
            ["í—¤ë“œë¼ì¸", "í•œêµ­", "êµ­ì œ", "ë¹„ì¦ˆë‹ˆìŠ¤Â·ê²½ì œ", "ê¸°ìˆ Â·IT", "ê³¼í•™", "ìŠ¤í¬ì¸ ", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "ê±´ê°•"],
            index=0
        )
        rss = google_news_rss_url(topic=topic)
    else:
        query = st.text_input("ê²€ìƒ‰ì–´ (ì˜ˆ: ë°˜ë„ì²´, ì´ì„ , AI ë°˜ë„ì²´)", value="AI")
        rss = google_news_rss_url(query=query)

    n_show = st.slider("í‘œì‹œí•  ê¸°ì‚¬ ê°œìˆ˜", min_value=3, max_value=30, value=10, step=1)
    n_sum = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", min_value=2, max_value=7, value=3, step=1)

    st.markdown("---")
    st.subheader("ìš”ì•½ ì˜µì…˜")
    bullet_mode = st.checkbox("í‚¤ ë¬¸ì¥ ë¶ˆë¦¿ìœ¼ë¡œ í‘œì‹œ", value=True)
    include_source_snippet = st.checkbox("RSS ìš”ì•½ë¬¸ë„ í•¨ê»˜ í‘œì‹œ", value=False)

    st.markdown("---")
    st.caption("â±ï¸ ë°ì´í„°ëŠ” 2~5ë¶„ ê°„ ìºì‹œë©ë‹ˆë‹¤.")

# --------- í—¤ë” ----------
st.markdown("## ğŸ—ï¸ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìš”ì•½ ì›¹")
if mode == "í† í”½ íƒìƒ‰":
    st.caption(f"ì¹´í…Œê³ ë¦¬: **{topic}** â€¢ ì†ŒìŠ¤: Google News RSS â€¢ íƒ€ì„ì¡´: KST")
else:
    st.caption(f"ê²€ìƒ‰ì–´: **{query}** â€¢ ì†ŒìŠ¤: Google News RSS â€¢ íƒ€ì„ì¡´: KST")

# --------- ë°ì´í„° ë¡œë“œ ----------
with st.spinner("ë‰´ìŠ¤ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
    items = fetch_feed(rss)
    cards = make_cards(items, n_show)

# --------- TOP 3 í•˜ì´ë¼ì´íŠ¸ ----------
top3 = cards[:3]
if top3:
    st.markdown("### â­ ì˜¤ëŠ˜ì˜ TOP 3")
    cols = st.columns(len(top3))
    for c, it in zip(cols, top3):
        with c:
            st.markdown(f"**[{it['title']}]({it['link']})**")
            st.caption(f"{it['source']} â€¢ {reltime(it['published'])}")

st.markdown("---")

# --------- ë³¸ë¬¸ & ìš”ì•½ ë Œë”ë§ ----------
export_rows = []
for it in cards:
    with st.container(border=True):
        left, right = st.columns([0.72, 0.28], vertical_alignment="start")
        with left:
            st.markdown(f"#### [{it['title']}]({it['link']})")
            st.caption(f"{it['source']} â€¢ {reltime(it['published'])}")
            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ & ìš”ì•½
            with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                article_text = fetch_article_text(it["link"])
                summary_sents = lexrank_summary(article_text, n_sum, language="korean")
            if summary_sents:
                if bullet_mode:
                    st.markdown("\n".join([f"- {s}" for s in summary_sents]))
                else:
                    st.write(" ".join(summary_sents))
            else:
                st.info("ë³¸ë¬¸ì„ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í•´ RSS ìš”ì•½ë¬¸ì„ ëŒ€ì‹  í‘œì‹œí•©ë‹ˆë‹¤.")
            if include_source_snippet and it["summary"]:
                with st.expander("ì›ë¬¸ RSS ìš”ì•½ ë³´ê¸°"):
                    st.write(it["summary"])
        with right:
            st.write("**í•µì‹¬ ì •ë³´**")
            st.write(f"- ì¶œì²˜: {it['source'] or 'ì•Œìˆ˜ì—†ìŒ'}")
            st.write(f"- ê²Œì‹œ: {to_kst(it['published']).strftime('%Y-%m-%d %H:%M')}")
            st.link_button("ê¸°ì‚¬ ì—´ê¸°", it["link"])

        # ë‹¤ìš´ë¡œë“œìš© ë ˆì½”ë“œ ì¶•ì 
        export_rows.append({
            "title": it["title"],
            "link": it["link"],
            "source": it["source"],
            "published_kst": to_kst(it["published"]).strftime('%Y-%m-%d %H:%M'),
            "summary": " ".join(summary_sents) if summary_sents else it["summary"],
        })

st.markdown("---")

# --------- ë‚´ë³´ë‚´ê¸° ----------
df_export = pd.DataFrame(export_rows)
col1, col2 = st.columns([0.5, 0.5])
with col1:
    st.write("### ğŸ“¤ ìš”ì•½ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
with col2:
    csv = df_export.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "CSV ë‹¤ìš´ë¡œë“œ",
        data=csv,
        file_name="news_summaries.csv",
        mime="text/csv"
    )

with st.expander("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
    st.dataframe(df_export, use_container_width=True)

# --------- í‘¸í„° ----------
st.caption(
    "â€» êµìœ¡ìš© ë°ëª¨ì…ë‹ˆë‹¤. ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” ë³¸ë¬¸ ì¶”ì¶œì´ ì œí•œë  ìˆ˜ ìˆì–´ RSS ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
    "ìš”ì•½ì€ LexRank(ì¶”ì¶œ ìš”ì•½)ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤."
)
